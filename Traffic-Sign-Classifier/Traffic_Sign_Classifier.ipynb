{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "training_file = \"train.p\"\n",
    "validation_file=\"valid.p\"\n",
    "testing_file = \"test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 34799\n",
      "Number of validation samples =  4410\n",
      "Number of testing samples = 12630\n",
      "Image data shape = (34799, 32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# Number of training examples\n",
    "n_train = len(train['features'])\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(valid['features'])\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(test['features'])\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = train['features'].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(numpy.unique(train['labels']))\n",
    "\n",
    "print(\"Number of training samples =\", n_train)\n",
    "print(\"Number of validation samples = \", n_validation)\n",
    "print(\"Number of testing samples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42934cea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique, counts = numpy.unique(train['labels'], return_counts=True)\n",
    "plt.bar(unique, counts)\n",
    "plt.savefig('distribution.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "oneHotTrain = to_categorical(y_train, num_classes=43)\n",
    "oneHotValid = to_categorical(y_valid, num_classes=43)\n",
    "oneHotTest = to_categorical(y_test, num_classes=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassign\n",
    "y_train = oneHotTrain\n",
    "y_valid = oneHotValid\n",
    "y_test = oneHotTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to grayscale\n",
    "X_trainGray = numpy.sum(X_train/3, axis=3, keepdims=True)\n",
    "X_testGray = numpy.sum(X_test/3, axis=3, keepdims=True)\n",
    "X_validGray = numpy.sum(X_valid/3, axis=3, keepdims=True)\n",
    "\n",
    "# Center and Normalize\n",
    "X_trainNorm = (X_trainGray - 128)/128 \n",
    "X_testNorm = (X_testGray - 128)/128\n",
    "X_validNorm = (X_validGray - 128)/128\n",
    "\n",
    "#reassign\n",
    "X_train = X_trainNorm\n",
    "X_test = X_testNorm\n",
    "X_valid = X_validNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGenerator=image.ImageDataGenerator(\n",
    "        shear_range=0.2, zoom_range=0.2,\n",
    "        rotation_range=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 43)                2795      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 43)                0         \n",
      "=================================================================\n",
      "Total params: 143,979\n",
      "Trainable params: 143,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, 3, padding='same', input_shape=(32,32,1), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(43))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1739/1739 [============================>.] - ETA: 0s - loss: 1.4118 - acc: 0.6015Epoch 00001: val_loss improved from inf to 0.47542, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 1.4116 - acc: 0.6015 - val_loss: 0.4754 - val_acc: 0.8694\n",
      "Epoch 2/20\n",
      "1738/1739 [============================>.] - ETA: 0s - loss: 0.4801 - acc: 0.8547Epoch 00002: val_loss improved from 0.47542 to 0.31667, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 29s 17ms/step - loss: 0.4799 - acc: 0.8547 - val_loss: 0.3167 - val_acc: 0.8946\n",
      "Epoch 3/20\n",
      "1739/1739 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9026Epoch 00003: val_loss improved from 0.31667 to 0.27431, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 31s 18ms/step - loss: 0.3260 - acc: 0.9026 - val_loss: 0.2743 - val_acc: 0.9293\n",
      "Epoch 4/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.2595 - acc: 0.9203Epoch 00004: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 19ms/step - loss: 0.2591 - acc: 0.9204 - val_loss: 0.2783 - val_acc: 0.9295\n",
      "Epoch 5/20\n",
      "1738/1739 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9323Epoch 00005: val_loss improved from 0.27431 to 0.24747, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 31s 18ms/step - loss: 0.2209 - acc: 0.9323 - val_loss: 0.2475 - val_acc: 0.9424\n",
      "Epoch 6/20\n",
      "1737/1739 [============================>.] - ETA: 0s - loss: 0.1993 - acc: 0.9393Epoch 00006: val_loss improved from 0.24747 to 0.22284, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 31s 18ms/step - loss: 0.1990 - acc: 0.9393 - val_loss: 0.2228 - val_acc: 0.9431\n",
      "Epoch 7/20\n",
      "1737/1739 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9455Epoch 00007: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1791 - acc: 0.9455 - val_loss: 0.3010 - val_acc: 0.9211\n",
      "Epoch 8/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9494Epoch 00008: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1659 - acc: 0.9495 - val_loss: 0.2512 - val_acc: 0.9331\n",
      "Epoch 9/20\n",
      "1738/1739 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9534Epoch 00009: val_loss improved from 0.22284 to 0.21147, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1578 - acc: 0.9534 - val_loss: 0.2115 - val_acc: 0.9449\n",
      "Epoch 10/20\n",
      "1738/1739 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9549Epoch 00010: val_loss did not improve\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 0.1510 - acc: 0.9550 - val_loss: 0.2385 - val_acc: 0.9401\n",
      "Epoch 11/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9571Epoch 00011: val_loss improved from 0.21147 to 0.19865, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1474 - acc: 0.9570 - val_loss: 0.1986 - val_acc: 0.9490\n",
      "Epoch 12/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9586Epoch 00012: val_loss improved from 0.19865 to 0.17503, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 0.1402 - acc: 0.9586 - val_loss: 0.1750 - val_acc: 0.9533\n",
      "Epoch 13/20\n",
      "1739/1739 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9585Epoch 00013: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 19ms/step - loss: 0.1349 - acc: 0.9585 - val_loss: 0.2095 - val_acc: 0.9463\n",
      "Epoch 14/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9614Epoch 00014: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1285 - acc: 0.9615 - val_loss: 0.2206 - val_acc: 0.9533\n",
      "Epoch 15/20\n",
      "1738/1739 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9629Epoch 00015: val_loss did not improve\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 0.1279 - acc: 0.9629 - val_loss: 0.2374 - val_acc: 0.9433\n",
      "Epoch 16/20\n",
      "1739/1739 [============================>.] - ETA: 0s - loss: 0.1183 - acc: 0.9645Epoch 00016: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1183 - acc: 0.9645 - val_loss: 0.2058 - val_acc: 0.9474\n",
      "Epoch 17/20\n",
      "1737/1739 [============================>.] - ETA: 0s - loss: 0.1186 - acc: 0.9649Epoch 00017: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1185 - acc: 0.9649 - val_loss: 0.1784 - val_acc: 0.9571\n",
      "Epoch 18/20\n",
      "1737/1739 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9632Epoch 00018: val_loss improved from 0.17503 to 0.15436, saving model to weights/model4.hdf5\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 0.1206 - acc: 0.9632 - val_loss: 0.1544 - val_acc: 0.9592\n",
      "Epoch 19/20\n",
      "1737/1739 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9654Epoch 00019: val_loss did not improve\n",
      "1740/1739 [==============================] - 33s 19ms/step - loss: 0.1143 - acc: 0.9654 - val_loss: 0.1664 - val_acc: 0.9567\n",
      "Epoch 20/20\n",
      "1736/1739 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9664Epoch 00020: val_loss did not improve\n",
      "1740/1739 [==============================] - 32s 18ms/step - loss: 0.1165 - acc: 0.9665 - val_loss: 0.1798 - val_acc: 0.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc3083a2e8>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='weights/model4.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(trainGenerator.flow(X_train, y_train, batch_size=20),\n",
    "                    steps_per_epoch=len(X_train) / 20, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid), callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 8s 656us/step\n",
      "test loss:  0.201709056157\n",
      "test acc:  0.950197933877\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights/model4.hdf5')\n",
    "score = model.evaluate(X_test, y_test, batch_size = 20)\n",
    "print(\"test loss: \", score[0])\n",
    "print(\"test acc: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
